{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from skimage.measure import regionprops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "from torch.optim import Adam\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Pore Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_3D=pickle.load(open(('Pore_Graph_for_H2.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer 3D geometric pore graph (RAG_3D) to the reduced topological pore graph(RAG_2D)\n",
    "RAG_2D=[]\n",
    "for g in RAG_3D:\n",
    "        nodes_2d=[]\n",
    "        features_2d=[]\n",
    "        cell_volume=[]\n",
    "        radius=[]\n",
    "        max_pbc_group = max(data[\"pbc_group\"] for _, data in g.nodes(data=True))\n",
    "        rag_2d=nx.MultiGraph()\n",
    "        for node in g.nodes:\n",
    "            node_2d=g.nodes[node]['pbc_group']\n",
    "\n",
    "            if node_2d not in nodes_2d:\n",
    "                nodes_2d.append(node_2d)\n",
    "                rag_2d.add_nodes_from(nodes_2d)\n",
    "                features_2d.append(g.nodes[node]['vdw_hist_y_pbc'])\n",
    "                cell_volume.append(g.nodes[node]['cell_volume'])\n",
    "                radius.append(g.nodes[node]['maxima_radii'])\n",
    "                node_labels= {i: {'labels': attr} for i, attr in enumerate(nodes_2d)}\n",
    "                node_eh= {i: {'eh': attr} for i, attr in enumerate(features_2d)}\n",
    "                node_cell_volume= {i: {'cell_volume': attr} for i, attr in enumerate(cell_volume)}\n",
    "                nx.set_node_attributes(rag_2d, node_cell_volume)\n",
    "                nx.set_node_attributes(rag_2d, node_labels)\n",
    "                nx.set_node_attributes(rag_2d, node_eh)\n",
    "\n",
    "            else:\n",
    "                if g.nodes[node]['maxima_radii']>radius[node_2d]:\n",
    "                    radius[node_2d]=g.nodes[node]['maxima_radii']\n",
    "                    \n",
    "            node_radius= {i: {'radius': attr} for i, attr in enumerate(radius)}\n",
    "            nx.set_node_attributes(rag_2d, node_radius)\n",
    "\n",
    "        pixel_ratios_2d=[0]*(max_pbc_group+1) \n",
    "        for node in g.nodes:\n",
    "            node_2d=g.nodes[node]['pbc_group']\n",
    "            pixel_ratio=g.nodes[node]['pixel_ratio']\n",
    "            pixel_ratios_2d[(node_2d)]=pixel_ratios_2d[(node_2d)]+pixel_ratio\n",
    "            \n",
    "        node_pixel_ratio= {i: {'pixel_ratio': attr} for i, attr in enumerate(pixel_ratios_2d)}\n",
    "        nx.set_node_attributes(rag_2d, node_pixel_ratio)\n",
    "            \n",
    "        RAG_2D.append(rag_2d)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devide the pore graphs into training, validation, and testing dataset\n",
    "RAG_Training=RAG_2D[0:900]\n",
    "RAG_Validation=RAG_2D[900:990]\n",
    "RAG_Testing=RAG_2D[990:1990] \n",
    "print([len(RAG_Training),len(RAG_Validation),len(RAG_Testing)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Graphs and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert graphs to DGL graphs and set node features\n",
    "def Process_Graphs_Labels(RAGs):\n",
    "  DGL_Graphs = []\n",
    "  for g in RAGs:\n",
    "    if g is not None:\n",
    "        DGL_g = dgl.from_networkx(g)\n",
    "        DGL_g = dgl.add_self_loop(DGL_g)  # Add self-loops\n",
    "        DGL_g.ndata['feat'] = torch.tensor([data['eh'] for _, data in g.nodes(data=True)])\n",
    "        DGL_g.ndata['maxima_radii'] = torch.tensor([data['radius'] for _, data in g.nodes(data=True)])\n",
    "        DGL_g.ndata['cell_volume'] = torch.tensor([data['cell_volume'] for _, data in g.nodes(data=True)])\n",
    "        DGL_g.ndata['pixel_ratio'] = torch.tensor([data['pixel_ratio'] for _, data in g.nodes(data=True)])\n",
    "        DGL_Graphs.append(DGL_g)\n",
    "  return DGL_Graphs\n",
    "\n",
    "# Get DGL graphs and Labels\n",
    "DGL_Graphs_Training=Process_Graphs_Labels(RAG_Training)\n",
    "DGL_Graphs_Validation=Process_Graphs_Labels(RAG_Validation)\n",
    "DGL_Graphs_Testing=Process_Graphs_Labels(RAG_Testing)\n",
    "\n",
    "# Convert list of DGL graphs into a single batched graph\n",
    "Batched_Graph_Training = dgl.batch(DGL_Graphs_Training)\n",
    "Batched_Graph_Validation=dgl.batch(DGL_Graphs_Validation)\n",
    "Batched_Graph_Testing = dgl.batch(DGL_Graphs_Testing)\n",
    "\n",
    "# Extract node features for all graphs\n",
    "Features_Training = Batched_Graph_Training.ndata['feat'].float()\n",
    "Features_Validation = Batched_Graph_Validation.ndata['feat'].float()\n",
    "Features_Testing = Batched_Graph_Testing.ndata['feat'].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab Cell Volumes, Pore Volume, and Pore Pixel Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grab_Cell_Volume(RAGs):\n",
    "    Cell_Volume=[]\n",
    "    for i,g in enumerate(RAGs):\n",
    "        vol=g.nodes[0]['cell_volume']\n",
    "        Cell_Volume.append(vol)\n",
    "    return torch.tensor(Cell_Volume).unsqueeze(1)\n",
    "\n",
    "Cell_Volume_Training=Grab_Cell_Volume(RAG_Training)\n",
    "Cell_Volume_Validation=Grab_Cell_Volume(RAG_Validation) \n",
    "Cell_Volume_Testing=Grab_Cell_Volume(RAG_Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grab_Pore_Volume(RAGs):\n",
    "    Pore_Volume=[]\n",
    "    for i,g in enumerate(RAGs):\n",
    "        for x in g.nodes:\n",
    "            cell_vol=g.nodes[x]['cell_volume']\n",
    "            pore_ratio=g.nodes[x]['pixel_ratio']\n",
    "            pore_vol=cell_vol*pore_ratio\n",
    "            Pore_Volume.append(pore_vol)\n",
    "    return torch.tensor(Pore_Volume).unsqueeze(1)\n",
    "\n",
    "Pore_Volume_Training=Grab_Pore_Volume(RAG_Training)\n",
    "Pore_Volume_Validation=Grab_Pore_Volume(RAG_Validation)\n",
    "Pore_Volume_Testing=Grab_Pore_Volume(RAG_Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Pixel_Ratio(RAGs):\n",
    " pixel_ratio_list=[]\n",
    " for i, G in enumerate(RAGs):\n",
    "    for n in sorted(G.nodes):\n",
    "        pixel_ratio_list.append(G.nodes[n]['pixel_ratio'])\n",
    " return pixel_ratio_list \n",
    "\n",
    "Pixel_Ratio_List_Training=torch.tensor(Extract_Pixel_Ratio(RAG_Training)).float().view(-1,1)\n",
    "Pixel_Ratio_List_Validation=torch.tensor(Extract_Pixel_Ratio(RAG_Validation)).float().view(-1,1)\n",
    "Pixel_Ratio_List_Testing=torch.tensor(Extract_Pixel_Ratio(RAG_Testing)).float().view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import MOF-level and Pore-level Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_all=pd.read_excel('.../PoroNet/GCMC_output/160K_5bar-mof-gL-0609.xlsx').to_numpy()[:,1].reshape(-1,1)\n",
    "\n",
    "Labels_Training_mof=Data_all[0:900]\n",
    "Labels_Validation_mof=Data_all[900:990]\n",
    "Labels_Testing_mof=Data_all[990:1990]\n",
    "\n",
    "Labels_Training_mof = np.array([[float(x[0])] for x in Labels_Training_mof])\n",
    "Labels_Validation_mof = np.array([[float(x[0])] for x in Labels_Validation_mof])\n",
    "Labels_Testing_mof = np.array([[float(x[0])] for x in Labels_Testing_mof])\n",
    "\n",
    "Labels_Training_mof=torch.tensor(Labels_Training_mof)\n",
    "Labels_Validation_mof=torch.tensor(Labels_Validation_mof)\n",
    "Labels_Testing_mof=torch.tensor(Labels_Testing_mof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of pores in the training, validation, and testing datasets\n",
    "Batched_Graph_Training,Batched_Graph_Validation, Batched_Graph_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_pore_all=pd.read_excel('.../PoroNet/GCMC_output/160K_5bar-pore-molecule-0609.xlsx').to_numpy()\n",
    "Data_Training_pore_molecule=Data_pore_all[0:6078]\n",
    "Data_Validation_pore_molecule=Data_pore_all[6078:6563]\n",
    "Data_Testing_pore_molecule=Data_pore_all[6563:13343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Labels(Data):\n",
    "  Labels = []\n",
    "  for data in Data:\n",
    "        Labels.append(data)\n",
    "  return Labels \n",
    "\n",
    "Labels_Training_pore_molecule=Process_Labels(Data_Training_pore_molecule)\n",
    "Labels_Validation_pore_molecule=Process_Labels(Data_Validation_pore_molecule)\n",
    "Labels_Testing_pore_molecule=Process_Labels(Data_Testing_pore_molecule)\n",
    "\n",
    "Labels_Training_pore_molecule = torch.tensor(Labels_Training_pore_molecule).float()\n",
    "Labels_Validation_pore_molecule = torch.tensor(Labels_Validation_pore_molecule).float()\n",
    "Labels_Testing_pore_molecule=torch.tensor(Labels_Testing_pore_molecule).float()\n",
    "\n",
    "Labels_Training_pore_gL=((Labels_Training_pore_molecule/(6.022E23))*2.01588)/Pore_Volume_Training\n",
    "Labels_Validation_pore_gL=((Labels_Validation_pore_molecule/(6.022E23))*2.01588)/Pore_Volume_Validation\n",
    "Labels_Testing_pore_gL=((Labels_Testing_pore_molecule/(6.022E23))*2.01588)/Pore_Volume_Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define PoroNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoroNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_sizes,hidden_activations,dropout):\n",
    "        super(PoroNet,self).__init__()\n",
    "        layers=[]\n",
    "        in_feat=input_size\n",
    "         \n",
    "        for i,hidden_size in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(in_feat,hidden_size))\n",
    "            hidden_activation=hidden_activations[i]\n",
    "            if hidden_activation=='relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            in_feat=hidden_size\n",
    "\n",
    "        layers.append(nn.Dropout(p=dropout))\n",
    "        layers.append(nn.Linear(in_feat,1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "         \n",
    "         \n",
    "    def forward(self,g,input_feature):\n",
    "        x=self.model(input_feature)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 fold\n",
    "cv=5\n",
    "\n",
    "#maximum of training epoch\n",
    "num_epoch=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the pore index of each MOF in the training set\n",
    "rag_label_dict = defaultdict(list)\n",
    "label_index=0\n",
    "for i, rag in enumerate(RAG_Training):\n",
    "    node_amounts=rag.number_of_nodes()\n",
    "    label = Labels_Training_pore_gL[label_index:label_index+node_amounts] \n",
    "    label_index += node_amounts\n",
    "    rag_label_dict[i].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters\n",
    "    num_patience=trial.suggest_int('num_patience', 10, 2000)\n",
    "    num_layers=trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_sizes = [trial.suggest_int(f'hidden_size{i}', 16, 128) for i in range(num_layers)]\n",
    "    hidden_activations=[trial.suggest_categorical(f'hidden_activation{i}',['relu','none'])for i in range(num_layers)]\n",
    "    dropout = trial.suggest_uniform('dropout', 0.2, 0.4)\n",
    "    initial_lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-9, 1e-3)\n",
    "    factor=trial.suggest_uniform('factor',0.1, 0.9)\n",
    "    \n",
    "\n",
    "    # K-Fold CV\n",
    "    kf = KFold(n_splits=cv, shuffle=False)\n",
    "    mae_list = []\n",
    "    for fold, (training_index, validation_index) in enumerate(kf.split(range(len(RAG_Training)))):\n",
    "    \n",
    "     # Extract pore graphs and labels according to index\n",
    "     RAG_training = [RAG_Training[i] for i in training_index]\n",
    "     RAG_validation = [RAG_Training[i] for i in validation_index]\n",
    "     Labels_training = [rag_label_dict[i] for i in training_index]\n",
    "     Labels_validation = [rag_label_dict[i] for i in validation_index]\n",
    "     \n",
    "     Labels_training_mof=Labels_Training_mof[training_index]\n",
    "     Labels_validation_mof=Labels_Training_mof[validation_index]\n",
    "     \n",
    "     # Obtain the dgl graphs\n",
    "     DGL_Graphs_training = Process_Graphs_Labels(RAG_training)\n",
    "     DGL_Graphs_validation = Process_Graphs_Labels(RAG_validation)\n",
    "     \n",
    "     #Revise the format of training labels\n",
    "     squeezed_tensors_training = []\n",
    "     for sublist in Labels_training:\n",
    "       for tensor in sublist:\n",
    "        squeezed_tensor = tensor.squeeze()  \n",
    "        if squeezed_tensor.ndimension() == 0: \n",
    "            squeezed_tensor = squeezed_tensor.unsqueeze(0).unsqueeze(1)\n",
    "        elif squeezed_tensor.ndimension() == 1: \n",
    "            squeezed_tensor = squeezed_tensor.unsqueeze(1)\n",
    "        squeezed_tensors_training.append(squeezed_tensor)\n",
    "\n",
    "     combined_tensor_training = torch.cat(squeezed_tensors_training, dim=0)\n",
    "     Labels_training = combined_tensor_training\n",
    "\n",
    "     #Revise the format of validation labels\n",
    "     squeezed_tensors_validation = []\n",
    "     for sublist in Labels_validation:\n",
    "       for tensor in sublist:\n",
    "        squeezed_tensor = tensor.squeeze()  \n",
    "        if squeezed_tensor.ndimension() == 0:  \n",
    "            squeezed_tensor = squeezed_tensor.unsqueeze(0).unsqueeze(1)\n",
    "        elif squeezed_tensor.ndimension() == 1: \n",
    "            squeezed_tensor = squeezed_tensor.unsqueeze(1)\n",
    "        squeezed_tensors_validation.append(squeezed_tensor)\n",
    "\n",
    "     combined_tensor_validation = torch.cat(squeezed_tensors_validation, dim=0)\n",
    "     Labels_validation = combined_tensor_validation\n",
    "\n",
    "     # Convert list of DGL graphs into a single batched graph\n",
    "     Batched_Graph_training = dgl.batch(DGL_Graphs_training)\n",
    "     Batched_Graph_validation=dgl.batch(DGL_Graphs_validation)\n",
    "\n",
    "     # Extract node features for all graphs\n",
    "     Features_training = Batched_Graph_training.ndata['feat'].float()\n",
    "     Features_validation = Batched_Graph_validation.ndata['feat'].float()\n",
    "     \n",
    "     Pixel_Ratio_List_training=torch.tensor(Extract_Pixel_Ratio(RAG_training)).float().view(-1,1)\n",
    "     Pixel_Ratio_List_validation=torch.tensor(Extract_Pixel_Ratio(RAG_validation)).float().view(-1,1)\n",
    "     \n",
    "     Cell_Volume_training=Grab_Cell_Volume(RAG_training)\n",
    "     Cell_Volume_validation=Grab_Cell_Volume(RAG_validation) \n",
    "\n",
    "     # Initialize the model and optimizer\n",
    "     model = PoroNet(Features_training.size(1),hidden_sizes,hidden_activations,dropout)\n",
    "     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=0.9* num_patience, verbose=True)\n",
    "        \n",
    "     # Initialize early stopping parameters\n",
    "     best_val_loss = float('inf') \n",
    "     stop_patience = num_patience  \n",
    "     counter = 0  \n",
    "     early_stopped = False  \n",
    "     best_epoch_loss = None      \n",
    "     \n",
    "     # Training loop\n",
    "     for epoch in range(num_epoch):\n",
    "         model.train()\n",
    "         Embeddings_training_1 = model(Batched_Graph_training,Features_training)  # Forward propagation, get embeddings\n",
    "         Embeddings_training_2 = Embeddings_training_1 * Pixel_Ratio_List_training\n",
    "         Batched_Graph_training.ndata['h'] = Embeddings_training_2  # Assign embeddings to 'h' field\n",
    "         Logits_training = dgl.sum_nodes(Batched_Graph_training, 'h')  # Sum the embeddings across nodes\n",
    "         Loss_training_pore = F.l1_loss(Embeddings_training_1, Labels_training)  # Compute loss\n",
    "         Loss_training_mof = F.l1_loss(Logits_training, Labels_training_mof)  # Compute loss\n",
    "         Loss_training=(100*Loss_training_pore)+Loss_training_mof\n",
    "         optimizer.zero_grad()\n",
    "         Loss_training.backward()\n",
    "         optimizer.step()\n",
    "        \n",
    "         # Set model to evaluation mode\n",
    "         model.eval()  \n",
    "         with torch.no_grad():\n",
    "          Embeddings_validation_1 = model(Batched_Graph_validation,Features_validation) \n",
    "          Embeddings_validation_2 = Embeddings_validation_1 * Pixel_Ratio_List_validation\n",
    "          Batched_Graph_validation.ndata['h'] = Embeddings_validation_2\n",
    "          Logits_validation = dgl.sum_nodes(Batched_Graph_validation, 'h')  \n",
    "          Loss_validation_pore = F.l1_loss(Embeddings_validation_1, Labels_validation)  \n",
    "          Loss_validation_mof = F.l1_loss(Logits_validation, Labels_validation_mof) \n",
    "          Loss_validation=(100*Loss_validation_pore)+Loss_validation_mof\n",
    "    \n",
    "          scheduler.step(Loss_validation)\n",
    "             \n",
    "         # Early stopping\n",
    "         if Loss_validation < best_val_loss:\n",
    "          best_val_loss = Loss_validation\n",
    "          best_epoch_loss = Loss_validation.item()\n",
    "          counter = 0\n",
    "         else:\n",
    "            counter += 1\n",
    "            \n",
    "         if counter >= stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            early_stopped = True\n",
    "            mae_list.append(best_epoch_loss)\n",
    "            break\n",
    "        \n",
    "     if not early_stopped:  # If not early stopped, record the loss of the last epoch\n",
    "            mae_list.append(Loss_validation.item())\n",
    "\n",
    "    mae_average = np.mean(mae_list) \n",
    "    \n",
    "\n",
    "    if trial.should_prune():\n",
    "     raise optuna.exceptions.TrialPruned()\n",
    "    mae_average=np.mean(mae_list)\n",
    "    trial.set_user_attr('mae_list', mae_list)\n",
    "\n",
    "    return mae_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "study = optuna.create_study(sampler=sampler,direction='minimize', pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Trial Number: ', trial.number)\n",
    "print('  Value: ', trial.value)\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value)) \n",
    "print('  Fold MAEs: ', trial.user_attrs['mae_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determined optimal hyperparameters\n",
    "num_patience=1070\n",
    "model = PoroNet(Features_Testing.size(1),hidden_sizes=[19,123],hidden_activations=['none','relu'],dropout=0.33571087092969193)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05905695348998198, weight_decay=3.3558614446106994e-08)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.79717142906145, patience=0.9*num_patience, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize early stopping parameters\n",
    "best_val_loss = float('inf') \n",
    "stop_patience =num_patience # Number of epochs to wait for improvement\n",
    "counter = 0  # Counter for patience  \n",
    "# Initialize the loss of training and validation set\n",
    "Loss_Training_List=[]\n",
    "Loss_Validation_List=[]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        Embeddings_Training_1 = model(Batched_Graph_Training,Features_Training) # Forward propagation, get embeddings\n",
    "        Embeddings_Training_2 = Embeddings_Training_1 * Pixel_Ratio_List_Training\n",
    "        Batched_Graph_Training.ndata['h'] = Embeddings_Training_2  # Assign embeddings to 'h' field\n",
    "        Logits_Training = dgl.sum_nodes(Batched_Graph_Training, 'h')  # Sum the embeddings across nodes\n",
    "        Loss_Training_pore = F.l1_loss(Embeddings_Training_1, Labels_Training_pore_gL)  # Compute loss\n",
    "        Loss_Training_mof = F.l1_loss(Logits_Training, Labels_Training_mof)  # Compute loss\n",
    "        Loss_Training=(100*Loss_Training_pore)+Loss_Training_mof\n",
    "        optimizer.zero_grad()\n",
    "        Loss_Training_List.append(Loss_Training.item())\n",
    "        Loss_Training.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "         Embeddings_Validation_1 = model(Batched_Graph_Validation,Features_Validation)  # Forward propagation, get embeddings\n",
    "         Embeddings_Validation_2 = Embeddings_Validation_1 * Pixel_Ratio_List_Validation\n",
    "         Batched_Graph_Validation.ndata['h'] = Embeddings_Validation_2\n",
    "         Logits_Validation = dgl.sum_nodes(Batched_Graph_Validation, 'h')\n",
    "         Loss_Validation_pore = F.l1_loss(Embeddings_Validation_1, Labels_Validation_pore_gL)\n",
    "         Loss_Validation_mof = F.l1_loss(Logits_Validation, Labels_Validation_mof)\n",
    "         Loss_Validation=(100*Loss_Validation_pore)+Loss_Validation_mof\n",
    "         Loss_Validation_List.append(Loss_Validation.item())\n",
    "         scheduler.step(Loss_Validation)\n",
    "        # Early stopping\n",
    "        if Loss_Validation < best_val_loss:\n",
    "         best_val_loss = Loss_Validation\n",
    "         counter = 0\n",
    "        else:\n",
    "         counter += 1\n",
    "        if counter >= stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "       \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model at the MOF level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    Predictions_Testing_pore = model(Batched_Graph_Testing,Features_Testing)  \n",
    "    Predictions_Testing_pore_2 = Predictions_Testing_pore * Pixel_Ratio_List_Testing\n",
    "    Batched_Graph_Testing.ndata['h'] = Predictions_Testing_pore_2  \n",
    "    Predictions_Testing_mof = dgl.sum_nodes(Batched_Graph_Testing, 'h')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate R² and MAE at the MOF level\n",
    "r2_mof = r2_score(Labels_Testing_mof, Predictions_Testing_mof)\n",
    "Loss_Testing_mof = F.l1_loss(Predictions_Testing_mof, Labels_Testing_mof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_mof,Loss_Testing_mof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model at the pore level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select pores with a diameter bigger than 3 Å(radius > 1.5 Å)\n",
    "def Screen_Big_Pore(Batch_Graph):\n",
    "    Big_Pore=[]\n",
    "    for i in range(Batch_Graph.num_nodes()):\n",
    "            pore_radius = Batch_Graph.ndata['maxima_radii'][i].item()\n",
    "            if pore_radius>1.5:\n",
    "                Big_Pore.append(i)\n",
    "    return Big_Pore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Big_Pore_Testing=Screen_Big_Pore(Batched_Graph_Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_Testing_big_pore=Predictions_Testing_pore[Big_Pore_Testing]\n",
    "Labels_Testing_big_pore=Labels_Testing_pore_gL[Big_Pore_Testing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate R² and MAE at the pore level\n",
    "r2_big_pore = r2_score(Predictions_Testing_big_pore, Labels_Testing_big_pore)\n",
    "Loss_Testing_big_pore=F.l1_loss(Predictions_Testing_big_pore, Labels_Testing_big_pore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_big_pore,Loss_Testing_big_pore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
